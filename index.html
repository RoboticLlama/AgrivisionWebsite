<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Agrivision</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
:root {
  --color-primary: #438951;
  --color-primary-dark: #4A644E;
  --color-bg: #DFEEE2;
  --color-surface: #D1E7D2;
  --color-accent-warn: #EE7D28;
  --color-text-main: #1f2933;
  --color-text-muted: #4b5563;
}

*,
*::before,
*::after { box-sizing: border-box; }

body {
  margin: 0;
  font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
  background: var(--color-bg);
  color: var(--color-text-main);
  padding-top: 260px; /* leave space for big header */
}

/* ========================= HEADER ========================= */

.site-header {
  position: fixed;
  top: 0; left: 0; right: 0;
  z-index: 50;
  background: var(--color-primary-dark);
  color: #ffffff;
  padding: 5rem 2rem;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  transition: padding .35s ease, box-shadow .35s ease;
}

.site-header .logo {
  font-size: clamp(3rem, 6vw, 4.5rem);
  font-weight: 800;
  color: #fff;
  text-decoration: none;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  transition: font-size .35s;
}
    .site-header .logo-sub {
  font-size: clamp(1rem);
  font-weight: 800;
  color: #fff;
  text-decoration: none;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  transition: font-size .35s;
}

/* ========================= NAV ========================= */

.nav-links {
  margin-top: 1.5rem;
  display: flex;
  gap: 1.5rem;
  flex-wrap: wrap;
  justify-content: center;
}

.nav-links a {
  color: #e5f3ea;
  text-decoration: none;
  font-size: 1rem;
  padding-bottom: 6px;
  position: relative;
}

.nav-links a.active::after {
  content: "";
  position: absolute;
  left: 50%;
  transform: translateX(-50%);
  bottom: 0px;
  width: 60%;
  height: 3px;
  background: var(--color-surface);
  border-radius: 3px;
}

/* Shrunk header */

.site-header.shrink {
  padding: .9rem 2rem;
  flex-direction: row;
  justify-content: space-between;
  box-shadow: 0 4px 12px rgba(0,0,0,.15);
}

.site-header.shrink .logo { font-size: 1.4rem; }
.nav-links.shrink-nav { margin-top: 0; }

/* ========================= SECTIONS ========================= */

section {
  max-width: 950px;
  margin: 5rem auto;
  padding: 2rem 1.5rem;
}

.card {
  background: var(--color-surface);
  border-radius: 12px;
  padding: 2rem;
  box-shadow: 0 10px 25px rgba(0,0,0,.06);
}

.tag {
  display: inline-block;
  padding: .25rem .7rem;
  border-radius: 999px;
  background: rgba(67,137,81,.18);
  color: var(--color-primary-dark);
  font-size: .8rem;
  font-weight: 700;
  text-transform: uppercase;
}

/* === Images / visualization grid === */

.card img {
  width: 100%;
  display: block;
  border-radius: 10px;
}

.image-grid {
  display: grid;
  grid-template-columns: repeat(2, 1fr); /* Forces 2 per row */
  gap: 2.2rem;                          /* Larger spacing */
  margin-top: 2rem;
}

.image-grid img {
  width: 100%;
  border-radius: 14px;
  cursor: pointer;
  transition: transform .25s ease, box-shadow .25s ease;
}

/* Optional: slight zoom on hover */
.image-grid img:hover {
  transform: scale(1.03);
  box-shadow: 0 10px 28px rgba(0,0,0,.18);
}

/* ========================= LIGHTBOX (POP-OUT IMAGE) ========================= */

#lightbox {
  display: none;               /* HIDDEN BY DEFAULT */
  position: fixed;
  inset: 0;
  background: rgba(0, 0, 0, 0.65);
  backdrop-filter: blur(4px);
  align-items: center;
  justify-content: center;
  padding: 1.5rem;
  z-index: 100;
}

#lightbox.active {
  display: flex;               /* show when active */
}

.lightbox-content {
  background: #fdfdfd;
  max-width: min(1100px, 100%);
  max-height: 90vh;
  border-radius: 14px;
  box-shadow: 0 20px 40px rgba(0,0,0,0.3);
  overflow: auto;
  padding: 1.5rem 1.75rem 1.75rem;
  position: relative;
}

.lightbox-img {
  width: 100%;
  border-radius: 10px;
  display: block;
}

.lightbox-caption {
  margin-top: 1rem;
  color: var(--color-text-main);
  line-height: 1.6;
}

.lightbox-close {
  position: absolute;
  top: 0.75rem;
  right: 0.9rem;
  border: none;
  background: transparent;
  color: #555;
  font-size: 1.7rem;
  line-height: 1;
  cursor: pointer;
}

/* ========================= FOOTER ========================= */

.site-footer {
  margin-top: 4rem;
  padding: 1.5rem;
  background: var(--color-primary-dark);
  color: #e9f4ec;
  text-align: center;
  font-size: .85rem;
}
  </style>
</head>

<body>

<header class="site-header">
  <a href="#" class="logo">AgriVision</a>
  <p class="logo-sub">By: Mason Bongo</p>
  <nav class="nav-links">
    <a href="#motivation">Problem & Motivation</a>
    <a href="#approach">Technical Approach</a>
    <a href="#viz">Visualizations</a>
    <a href="#takeaways">Takeaways</a>
    <a href="#results">Results</a>
  </nav>
</header>

<section id="motivation" class="card">
  <span class="tag">01 — PROBLEM & MOTIVATION</span>
  <h2>What drove this work?</h2>
  <p>Ever since Huanglongbing (HLB), more commonly known as Citrus Greening, infiltrated the global citrus ecosystem, it has inflicted unprecedented damage on production, market stability, and agricultural sustainability. Its impact has been felt worldwide, but nowhere more severely than in the state of Florida. Since the disease was first documented there in 2005, the state has suffered an estimated 80% reduction in citrus yield, with economic losses exceeding four billion dollars. Entire groves have been rendered non-productive, family operations have collapsed, and commercial growers continue to struggle against a disease that spreads silently and aggressively.</p>
  <p>Our project, Agrivision, is driven by the urgent need for faster and more efficient early-stage detection. HLB is devastating not only because it is incurable, but because it progresses quietly — often remaining invisible to the human eye for several years. By the time visible symptoms emerge, the disease has already spread throughout the tree and, frequently, into surrounding groves. Traditional identification can take 2 to 5 years, making current approaches far too slow for a problem where every month — even every season — matters. Each undetected tree presents a new source of infection, multiplying economic impact and threatening industry survival.</p>
  <p>Our motivation is simple but ambitious: to give citrus growers a tool that recognizably identifies HLB before permanent damage and large-scale spread occur. With the integration of AlexNet, we explore whether deep learning can detect subtle visual indicators of infection far earlier than human observation allows. A system like this could enable growers to respond proactively rather than reactively — removing infected trees, protecting healthy ones, and preserving yield that would otherwise be lost.</p>
  <p>Looking forward, we envision Agrivision evolving into a scalable, field-ready platform capable of supporting growers across multiple citrus varieties and geographic regions. If successful, this technology could slow disease transmission, restore market output, and safeguard citrus agriculture for future generations.</p>
</section>

<section id="approach" class="card">
  <span class="tag">02 — TECHNICAL APPROACH</span>
  <h2>How did you solve it?</h2>
  <p>Our technical approach focused on evaluating whether AlexNet, a deep convolutional neural network originally developed for large-scale image classification, could learn visual indicators of Huanglongbing (HLB) in citrus leaves. The goal was not only to train a model, but to understand which configurations, preprocessing strategies, and hyperparameters maximized early-stage disease recognition.</p>
  <p>To begin, images were collected and organized into two classes — healthy and infected. The dataset was stored on Google Drive and imported into Google Colab for training. AlexNet requires input images of 224×224, so all samples were resized and normalized using ImageNet mean and variance distributions. Because available data was extremely limited (≈58 images total), we additionally applied light augmentation (horizontal flips, small rotations, and color jitter) in selected experiments to simulate natural field variation and reduce overfitting.</p>
  <p>Training was repeated across multiple controlled experiments to systematically measure how key parameters influence model behavior. First, we established a pretrained baseline, leveraging ImageNet weights to accelerate feature extraction. This was followed by an untrained AlexNet trial, where weights were randomly initialized to assess the value of transfer learning. We then explored RGB vs grayscale input, testing whether color contributes meaningful information to HLB prediction or if structural leaf texture is more relevant. We also varied batch size (16 vs 64) to analyze gradient stability, and learning rate (1e-3 vs 1e-5) to observe convergence behavior. Finally, we performed an architectural comparison using ResNet-18, allowing us to evaluate whether a deeper network improves classification.</p>
  <p>Each run logged training/validation accuracy, loss, and confusion matrices. Graphs clearly showed that AlexNet learned rapidly and achieved >90% training accuracy in most trials, confirming its feature extraction capability. However, validation accuracy remained low (25–42%), signaling overfitting linked to dataset size rather than model weakness. Grayscale input, lower learning rate, and larger batch size yielded the strongest generalization trends, while RGB and high-LR models were unstable and most prone to collapse.</p>
  <p>This work demonstrates that deep learning — and AlexNet specifically — is a viable pathway for citrus disease detection, but success at real-world scale will require substantially more imaging data and continued refinement.</p>
</section>

<!-- =============== VISUALIZATIONS SECTION =============== -->
<section id="viz" class="card">
  <span class="tag">03 — VISUALIZATIONS</span>
  <h2>Visualizations</h2>

  <div class="image-grid">
    <img src="Results/Epoch 1.png"          alt="Epoch 1 result"
         data-caption="Alexnet_bs16_lr1e-4_noaug – The model learns rapidly and reaches high training accuracy, while validation accuracy stays around 42%. Validation loss climbs steadily even as training loss falls, demonstrating strong overfitting. Without augmentation, the network mainly memorizes the training set instead of learning features that generalize">
    <img src="Results/Epoch 2.png"          alt="Epoch 2 result"
         data-caption="alexnet_bs64_lr1e-4_aug - Despite augmentation and a larger batch size, training accuracy again climbs above 90%, but validation accuracy drops to roughly 33%. While augmentation should help generalization, the extremely small dataset limits the benefit. Overfitting behavior is still clear.">
    <img src="Results/Epoch 3.png"          alt="Epoch 3 result"
         data-caption="alexnet_bs32_lr1e-3_aug - Increasing the learning rate to 1e-3 makes training less stable. Both training and validation curves fluctuate more, and validation accuracy remains low. This confirms that 1e-3 is too aggressive for this dataset and configuration and does not improve generalization.">
    <img src="Results/Epoch 4.png"          alt="Epoch 4 result"
         data-caption="resnet18_bs32_lr1e-4_aug - ResNet18 learns quickly and reaches high training accuracy, but validation accuracy again plateaus near 42%, similar to Experiment 1. This suggests that a deeper architecture does not solve generalization issues with a very limited dataset. Overfitting remains the dominant behavior. ">
  <img src="Results/Epoch pretrained.png" alt="Pretrained epoch result"
       data-caption="The pretrained model learns the training data extremely well, but generalization remains weak. Validation accuracy stalls around 42% and validation loss increases, indicating overfitting. This baseline sets the reference point for subsequent experiments.">
  <img src="Results/Epoch untrained.png"  alt="Untrained epoch result"
       data-caption=" The untrained model reaches much lower training accuracy than the pretrained baseline, confirming that transfer learning from ImageNet significantly accelerates learning. However, validation accuracy is nearly identical (~42%) in both cases, so generalization is still limited by data size. The smaller gap between train and validation metrics suggests the untrained model overfits less, but also learns less.">
  <img src="Results/Epoch RGB.png"        alt="RGB epoch result"
       data-caption="The RGB model achieves strong memorization of the training set but performs the worst on validation accuracy (25%) with very high validation loss. In contrast, using grayscale inputs results in slightly better validation accuracy (33%) and lower validation loss. Removing color information appears to encourage the network to rely more on edges and shapes, which generalize better in this small dataset.">
  <img src="Results/Epoch BW.png"         alt="Black and white epoch result"
       data-caption=" The RGB model achieves strong memorization of the training set but performs the worst on validation accuracy (25%) with very high validation loss. In contrast, using grayscale inputs results in slightly better validation accuracy (33%) and lower validation loss. Removing color information appears to encourage the network to rely more on edges and shapes, which generalize better in this small dataset.">
  <img src="Results/Epoch BS16.png"       alt="Epoch with batch size 16"
       data-caption="Both batch sizes reach very high training accuracy, showing that AlexNet can memorize the set easily. However, the larger batch size (64) achieves higher validation accuracy (about 41.7%). Larger batches produce smoother gradient estimates and slightly more stable optimization, leading to better generalization on this dataset, even though overfitting is still present.">
  <img src="Results/Epoch BS64.png"       alt="Epoch with batch size 64"
       data-caption="Both batch sizes reach very high training accuracy, showing that AlexNet can memorize the set easily. However, the larger batch size (64) achieves higher validation accuracy (about 41.7%). Larger batches produce smoother gradient estimates and slightly more stable optimization, leading to better generalization on this dataset, even though overfitting is still present.">
  <img src="Results/Epoch LRLE 3.png"     alt="Epoch with LR/LE 3"
       data-caption="The higher learning rate leads to unstable training, lower accuracy, and poorer validation performance. The optimizer likely overshoots minima and cannot settle into a good solution. With a smaller learning rate, the model trains more slowly but achieves better validation accuracy (~41.7%) and more stable loss curves. This indicates that a lower LR is more appropriate for this small dataset.">
  <img src="Results/Epoch LRLE 5.png"     alt="Epoch with LR/LE 5"
       data-caption="The higher learning rate leads to unstable training, lower accuracy, and poorer validation performance. The optimizer likely overshoots minima and cannot settle into a good solution. With a smaller learning rate, the model trains more slowly but achieves better validation accuracy (~41.7%) and more stable loss curves. This indicates that a lower LR is more appropriate for this small dataset.">
  </div>
</section>

<section id="takeaways" class="card">
  <span class="tag">04 — TAKEAWAYS</span>
  <h2>What did we learn?</h2>
  <p>The experiments collectively highlight an important finding: the architectures themselves are not the limiting factor — data quantity and variability are. AlexNet and ResNet both learned the training distribution extremely well, achieving high accuracy and low loss in most configurations. However, validation accuracy stayed consistently low across models, averaging between 33%–42% in the best cases. This indicates that the networks were able to memorize input patterns rather than extract general features capable of transferring to new samples. The behavior held true across learning rate shifts, batch size changes, augmentation strategies, and even complete architecture swaps.</p>
  <p>Additionally, the experiments revealed which configurations were most resilient under data constraints. Lower learning rates improved generalization stability, and grayscale outperformed RGB in validation accuracy, suggesting that color may add noise rather than useful information in this context. Larger batch sizes provided slight improvements in validation performance compared to smaller ones, likely due to smoother gradients and reduced update volatility. While none solved overfitting on their own, each experiment contributed insight into how the model behaves under constraint and how future scaling should be approached.</p>
</section>

<section id="results" class="card">
  <span class="tag">05 — RESULTS</span>
  <h2>Final conclusions</h2>
  <p>The overall results demonstrate that convolutional neural networks like AlexNet and ResNet are capable of learning the visual patterns associated with HLB disease symptoms, but reliable deployment will require more representative data. With only 58 total images, the models reached a performance ceiling early and could not generalize beyond the training set. Adjustments in batch size, learning rate, or input representation produced meaningful behavioral differences, yet validation accuracy remained clustered within a narrow range. This reinforces that the core performance bottleneck is dataset size, not model architecture or training strategy.</p>
  <p>With continued dataset expansion, improved field sampling, and stronger augmentation pipelines, accuracy could scale substantially. Future work may introduce patch-based training, multi-model ensembles, or region-of-interest feature cropping to increase signal density per image. Integrating spectral imaging or temporal orchard scans could further elevate classifier reliability. Although current models show promising early capability, Agrivision’s next stage will focus on real-world scalability — transforming laboratory performance into field-ready HLB detection that can protect Florida citrus for years to come.</p>
</section>

<footer class="site-footer">© 2025 AgriVision. All rights reserved.</footer>

<!-- ========= LIGHTBOX HTML ========= -->
<div id="lightbox">
  <div class="lightbox-content">
    <button class="lightbox-close" aria-label="Close image">&times;</button>
    <img id="lightbox-img" class="lightbox-img" src="" alt="">
    <p id="lightbox-caption" class="lightbox-caption"></p>
  </div>
</div>

<script>
const header     = document.querySelector(".site-header");
const navLinks   = document.querySelector(".nav-links");
const navAnchors = document.querySelectorAll(".nav-links a");
const sections   = document.querySelectorAll("section");

let isShrunk = false;

/* figure out which section is around mid-screen */
function updateActiveLink() {
  const viewportMiddle = window.scrollY + window.innerHeight / 2;
  let activeIndex = 0;

  sections.forEach((section, idx) => {
    const top = section.offsetTop;
    const bottom = top + section.offsetHeight;
    if (viewportMiddle >= top && viewportMiddle < bottom) {
      activeIndex = idx;
    }
  });

  navAnchors.forEach((link, idx) => {
    link.classList.toggle("active", idx === activeIndex);
  });
}

/* initial underline */
updateActiveLink();

/* shrink header as soon as you scroll at all */
window.addEventListener("scroll", () => {
  const shouldShrink = window.scrollY > 0;

  if (shouldShrink !== isShrunk) {
    isShrunk = shouldShrink;
    header.classList.toggle("shrink", isShrunk);
    navLinks.classList.toggle("shrink-nav", isShrunk);
  }

  updateActiveLink();
});

/* nav clicks: force shrunk header + scroll with offset */
navAnchors.forEach(link => {
  link.addEventListener("click", e => {
    e.preventDefault();
    const target = document.querySelector(link.getAttribute("href"));
    if (!target) return;

    if (!isShrunk) {
      isShrunk = true;
      header.classList.add("shrink");
      navLinks.classList.add("shrink-nav");
    }

    const headerHeight = header.offsetHeight;
    const offset = headerHeight + 40;
    const topPos = target.offsetTop - offset;

    window.scrollTo({ top: topPos, behavior: "smooth" });
  });
});

/* ========= LIGHTBOX SCRIPT ========= */

const lightbox        = document.getElementById("lightbox");
const lightboxImg     = document.getElementById("lightbox-img");
const lightboxCaption = document.getElementById("lightbox-caption");
const lightboxClose   = document.querySelector(".lightbox-close");
const gridImages      = document.querySelectorAll(".image-grid img");

function openLightbox(img) {
  lightboxImg.src = img.src;
  lightboxImg.alt = img.alt || "";
  lightboxCaption.textContent = img.dataset.caption || "";
  lightbox.classList.add("active");
}

function closeLightbox() {
  lightbox.classList.remove("active");
  lightboxImg.src = "";
}

/* open on image click */
gridImages.forEach(img => {
  img.addEventListener("click", () => openLightbox(img));
});

/* close on X */
lightboxClose.addEventListener("click", closeLightbox);

/* close when clicking outside content */
lightbox.addEventListener("click", (e) => {
  if (e.target === lightbox) {
    closeLightbox();
  }
});

/* close on Esc key */
document.addEventListener("keydown", (e) => {
  if (e.key === "Escape" && lightbox.classList.contains("active")) {
    closeLightbox();
  }
});
</script>

</body>
</html>
